{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sitting-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GroupKFold, GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.custom_selector import ColumnSelector\n",
    "from src.evaluate import EvaluateModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-suspect",
   "metadata": {},
   "source": [
    "# 0. Report\n",
    "## 0.1. Approach\n",
    "1. An in-depth analysis of the data was conducted to identify any issues with the quality of the data.<u>(Please refer to sections 1/2/3)</u>\n",
    "2. Next, an automated algorithm based on pearson's correlation and ANOVA's f-score is used to filter out irrelevant or duplicated features <u>(Please refer to section 4)</u>\n",
    "3. The data is split in train/test while ensuring that all vectors for a specific receipt are in the same split. This is necessary as all features are based on information from both the receipt image and the transaction. <u>(Please refer to section 5.)</u>\n",
    "4. A tree-based boosting algorithm (XGBoost) is used as a classifier. This is a perfect algorithm for the current task, as it can work very well with ordinal data and has high performance of imbalanced datasets. <u>(Please refer to section 6.)</u>\n",
    "5. The hyperparameters of the model are then optimized using a randomized search across pre-defined search space. Due to time constraints this approach was selected instead of using hyperparameter optimizers like hyperopt, which rely on bayesian optimization to find the optimal set of hyperparameters. The hyperparameter tuning is done on the training data by utilizing a cross-validation approach due to limited number of \"positive\" events. <u>(Please refer to section 6.)</u>\n",
    "6. The final model is trained on the full training data and is evaluated on the test data <u>(Please refer to section 6.)</u>\n",
    "7. Finally all steps are incorporated within a train.py script available in the root\n",
    "8. Please note that the model doesn't return a list, but an array with the original order of the data input. It is not advised for a model to return output in order different from its input, so for this project I would approach the data architect to revise the specification and impelment the ordering outside of the model \n",
    "\n",
    "## 0.2. Results\n",
    "1. The discriminatory power of the model is very high with <u>Gini on both Train and Test higher than 93%</u>. This means that the model performs very well in ordering the matching vectors <u>(Please refer to section 7.)</u>\n",
    "2. Distribution of the final score looks good, with observed values across the full range between 0 and 1. This means that we don't have huge clumps of values in a single score, which could complicate the discrimination. In addition it shows that even without calibration the probability score can serve as an indication of the true probability.\n",
    "3. The Final success rate is reported using 4 metrics:\n",
    "- <b>\"User Success Rate\" = 61%</b> - This is the success rate of visualziing the correct transaction in the final app as experienced by the user\n",
    "- <b>\"Maximum Success Rate\" = 75%</b> - This is the maximum success rate given that for a large proportion of the data we don't have the correct transaction supplied and assuming that when we have same explanatory features for 2 transactions, one of which is the correct one, we will select the correct one by chance\n",
    "- <b>\"Normaized Success Rate\" = 81%</b> - This is the success rate of the model after accounting for the receipts with missing matched transactions\n",
    "- <b>\"Normaized For Missing and Duplicates Success Rate\" = 91%</b> - This is the cussess rate after account for the receipts with missing matched transactions and after excluding all receipts for which we have same features for both the matched transaction and an incorrect transaction.\n",
    "\n",
    "All 4 above metrics are useful and they highlight the 2 main sources leading to lower model performance:\n",
    "- The quality of the data. It's impact can be seen by comparing the \"User Cussess Rate\" and the \"normalized For Missing and Duplicates Success Rate\"\n",
    "- Model specification and noise\n",
    "\n",
    "## 0.3. Below you can find a list of next steps to improve the model\n",
    "1. Contact the data supplier and discuss how to improve the current data extracts, as 26% of the supplied receipts don't have a matched transaction. <u>(Please refer to section 2.9)</u>\n",
    "2. Request higher granularity for some of the input features as for 22% of all receipts we have a matched transaction with exactly the same features as at least one another <u>(Please refer to section 2.9)</u>\n",
    "3. This means that in total 48% of the receipts can't be mapped consistently or at all, so it should be of highest priority to improve the quality of the data <u>(Please refer to section 2.9)</u>\n",
    "4. The hyperparameter tuning algorithm can be improved by utilizing package \"hyperopt\" (available in the pip index).\n",
    "5. We can introduce a threshold to exclude transaction with very low estiamted model confidence from the returned results. This is expected to improve the user experience.\n",
    "6. To do so, we should obtain a new independent dataset, apply the model on it, order it by predicted probability and see if below a certain threshold we don't see any matched transaction ids. This would speed up the process of validation for the app user.\n",
    "7. In addition we can increase the interpretability of the final score estimates by calibrating the resutls on the new independent dataset from the above point. For that we can use sklearn.calibration.CalibratedClassifierCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-drawing",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graduate-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data_interview_test_updated_(1).csv\", delimiter=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romantic-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (12034, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data dimensions:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "musical-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3740</th>\n",
       "      <th>4179</th>\n",
       "      <th>7797</th>\n",
       "      <th>10603</th>\n",
       "      <th>11195</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>receipt_id</th>\n",
       "      <td>20,132</td>\n",
       "      <td>20,173</td>\n",
       "      <td>30,244</td>\n",
       "      <td>40,273</td>\n",
       "      <td>50,094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_id</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>30000</td>\n",
       "      <td>40000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_transaction_id</th>\n",
       "      <td>20,411</td>\n",
       "      <td>20,180</td>\n",
       "      <td>30,639</td>\n",
       "      <td>40,115</td>\n",
       "      <td>50,334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_transaction_id</th>\n",
       "      <td>20,409</td>\n",
       "      <td>20,175</td>\n",
       "      <td>30,096</td>\n",
       "      <td>40,030</td>\n",
       "      <td>50,026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateMappingMatch</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmountMappingMatch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DescriptionMatch</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DifferentPredictedTime</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeMappingMatch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PredictedNameMatch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShortNameMatch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DifferentPredictedDate</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PredictedAmountMatch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PredictedTimeCloseMatch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          3740    4179    7797    10603   11195\n",
       "receipt_id               20,132  20,173  30,244  40,273  50,094\n",
       "company_id                20000   20000   30000   40000   50000\n",
       "matched_transaction_id   20,411  20,180  30,639  40,115  50,334\n",
       "feature_transaction_id   20,409  20,175  30,096  40,030  50,026\n",
       "DateMappingMatch           0.75       0       0       0       0\n",
       "AmountMappingMatch            0       0       0       0       0\n",
       "DescriptionMatch            0.6       0       0       0       0\n",
       "DifferentPredictedTime        1       1       1       1       1\n",
       "TimeMappingMatch              0       0       0       0       0\n",
       "PredictedNameMatch            0       0       0       0       0\n",
       "ShortNameMatch                0       0       0       1       0\n",
       "DifferentPredictedDate        0       1       1       1       1\n",
       "PredictedAmountMatch          0       0       0       0       0\n",
       "PredictedTimeCloseMatch       0       0       1       0       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5, random_state=21).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-oklahoma",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "\n",
    "1. The supplied data includes 12,034 data rows and 14 columns\n",
    "2. The first 4 features ensure the uniqueness of the data row, while the rest 10 features describe the feature_transaction_id realation to receipt_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-arena",
   "metadata": {},
   "source": [
    "# 2. Analyze Data Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-authentication",
   "metadata": {},
   "source": [
    "## 2.1. Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "consecutive-express",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "receipt_id                  object\n",
       "company_id                   int64\n",
       "matched_transaction_id      object\n",
       "feature_transaction_id      object\n",
       "DateMappingMatch           float64\n",
       "AmountMappingMatch         float64\n",
       "DescriptionMatch           float64\n",
       "DifferentPredictedTime     float64\n",
       "TimeMappingMatch           float64\n",
       "PredictedNameMatch         float64\n",
       "ShortNameMatch             float64\n",
       "DifferentPredictedDate     float64\n",
       "PredictedAmountMatch       float64\n",
       "PredictedTimeCloseMatch    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-prize",
   "metadata": {},
   "source": [
    "## 2.2. Check For Missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-parameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "receipt_id                 0\n",
       "company_id                 0\n",
       "matched_transaction_id     0\n",
       "feature_transaction_id     0\n",
       "DateMappingMatch           0\n",
       "AmountMappingMatch         0\n",
       "DescriptionMatch           0\n",
       "DifferentPredictedTime     0\n",
       "TimeMappingMatch           0\n",
       "PredictedNameMatch         0\n",
       "ShortNameMatch             0\n",
       "DifferentPredictedDate     0\n",
       "PredictedAmountMatch       0\n",
       "PredictedTimeCloseMatch    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-surfing",
   "metadata": {},
   "source": [
    "## 2.3. Check For Leading/Trailing WhiteSpaces in Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electric-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = data.dtypes[data.dtypes == \"object\"].index.values\n",
    "\n",
    "for column in string_columns:\n",
    "    if not data[column].equals(data[column].str.strip()):\n",
    "        raise Exception(f\"For column '{column}' we see entries with leading/trailing white space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-kidney",
   "metadata": {},
   "source": [
    "## 2.4. Check For uniquness of receipt_id per company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "designed-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.drop_duplicates([\"receipt_id\", \"company_id\"]).shape[0] != data.drop_duplicates([\"receipt_id\"]).shape[0]:\n",
    "    raise Exception(\"Ensure that receipt_id is uniue across all companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-italy",
   "metadata": {},
   "source": [
    "## 2.5. Check For duplicates across receipt_id/feature_transaction_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seeing-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.duplicated([\"receipt_id\", \"feature_transaction_id\"]).sum() > 0:\n",
    "    raise Exception(\"Duplicate matching vectors are observed in the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-somalia",
   "metadata": {},
   "source": [
    "## 2.6. Check For mismatch between receipt_id and matched_transaction_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "active-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (data[\"receipt_id\"].nunique() == data[\"matched_transaction_id\"].nunique() == data[[\"receipt_id\", \"matched_transaction_id\"]].drop_duplicates().shape[0]):\n",
    "    raise Exception(\"Please ensure that a single receipt_id is matched to a single matched_transaction_id and the opposite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-holly",
   "metadata": {},
   "source": [
    "## 2.7. Check Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fixed-vietnamese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==========DateMappingMatch=============\n",
      "0.000    9068\n",
      "0.950    1636\n",
      "0.850     571\n",
      "0.900     217\n",
      "0.650     194\n",
      "0.825     179\n",
      "0.550      98\n",
      "1.000      36\n",
      "0.750      21\n",
      "0.525      11\n",
      "0.725       3\n",
      "Name: DateMappingMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========AmountMappingMatch=============\n",
      "0.0    11225\n",
      "0.4      615\n",
      "0.7      159\n",
      "0.6       26\n",
      "0.9        9\n",
      "Name: AmountMappingMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========DescriptionMatch=============\n",
      "0.0    11581\n",
      "0.8      193\n",
      "0.4      143\n",
      "0.6       60\n",
      "0.2       57\n",
      "Name: DescriptionMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========DifferentPredictedTime=============\n",
      "1.0    11871\n",
      "0.0      163\n",
      "Name: DifferentPredictedTime, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========TimeMappingMatch=============\n",
      "0.0    11867\n",
      "1.0      167\n",
      "Name: TimeMappingMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========PredictedNameMatch=============\n",
      "0.0    11589\n",
      "0.8      251\n",
      "0.4       91\n",
      "0.6       84\n",
      "0.2       19\n",
      "Name: PredictedNameMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========ShortNameMatch=============\n",
      "0.0    11578\n",
      "1.0      456\n",
      "Name: ShortNameMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========DifferentPredictedDate=============\n",
      "1.0    9068\n",
      "0.0    2966\n",
      "Name: DifferentPredictedDate, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========PredictedAmountMatch=============\n",
      "0.0    11989\n",
      "0.1       24\n",
      "0.5        9\n",
      "0.4        8\n",
      "0.6        3\n",
      "0.2        1\n",
      "Name: PredictedAmountMatch, dtype: int64\n",
      "==================================================\n",
      "==================================================\n",
      "==========PredictedTimeCloseMatch=============\n",
      "0.0    11113\n",
      "1.0      921\n",
      "Name: PredictedTimeCloseMatch, dtype: int64\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "float_columns = data.dtypes[data.dtypes == \"float\"].index.values\n",
    "\n",
    "for column in float_columns:\n",
    "    print(\"==================================================\")\n",
    "    print(f\"=========={column}=============\")\n",
    "    distribution = data[column].value_counts()\n",
    "    \n",
    "    if len(distribution) < 20:\n",
    "        print(distribution)\n",
    "    else:\n",
    "        print(data[column].describe())\n",
    "    print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-oasis",
   "metadata": {},
   "source": [
    "## 2.8. Check Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "invisible-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = data[float_columns].corr().abs().stack().reset_index()\n",
    "corrs.columns = ['feature_1', 'feature_2', 'abs_corr']\n",
    "corrs = corrs[(corrs.feature_1 != corrs.feature_2)].sort_values(\"abs_corr\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "statutory-stanley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>abs_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>DifferentPredictedDate</td>\n",
       "      <td>DateMappingMatch</td>\n",
       "      <td>0.990860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DateMappingMatch</td>\n",
       "      <td>DifferentPredictedDate</td>\n",
       "      <td>0.990860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>TimeMappingMatch</td>\n",
       "      <td>0.987785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TimeMappingMatch</td>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>0.987785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>PredictedTimeCloseMatch</td>\n",
       "      <td>0.407039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TimeMappingMatch</td>\n",
       "      <td>PredictedAmountMatch</td>\n",
       "      <td>0.005925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>PredictedAmountMatch</td>\n",
       "      <td>0.005852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>PredictedAmountMatch</td>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>0.005852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>PredictedAmountMatch</td>\n",
       "      <td>DescriptionMatch</td>\n",
       "      <td>0.002102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DescriptionMatch</td>\n",
       "      <td>PredictedAmountMatch</td>\n",
       "      <td>0.002102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature_1                feature_2  abs_corr\n",
       "70  DifferentPredictedDate         DateMappingMatch  0.990860\n",
       "7         DateMappingMatch   DifferentPredictedDate  0.990860\n",
       "34  DifferentPredictedTime         TimeMappingMatch  0.987785\n",
       "43        TimeMappingMatch   DifferentPredictedTime  0.987785\n",
       "39  DifferentPredictedTime  PredictedTimeCloseMatch  0.407039\n",
       "..                     ...                      ...       ...\n",
       "48        TimeMappingMatch     PredictedAmountMatch  0.005925\n",
       "38  DifferentPredictedTime     PredictedAmountMatch  0.005852\n",
       "83    PredictedAmountMatch   DifferentPredictedTime  0.005852\n",
       "82    PredictedAmountMatch         DescriptionMatch  0.002102\n",
       "28        DescriptionMatch     PredictedAmountMatch  0.002102\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-visiting",
   "metadata": {},
   "source": [
    "## 2.9. Check for duplicate explanatory features for same receipt after discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "american-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for rec_id, df in data.groupby(\"receipt_id\"):\n",
    "    mask = df[\"matched_transaction_id\"] == df[\"feature_transaction_id\"]\n",
    "    \n",
    "    correct_tx = df[mask][float_columns]\n",
    "    incorrect_tx = df[~mask][float_columns]\n",
    "    \n",
    "    duplicate_flag = int(correct_tx.merge(incorrect_tx).shape[0] > 0)\n",
    "    \n",
    "    result.append({\n",
    "        \"receipt_id\": rec_id,\n",
    "        \"missing_target\": 1-int(mask.max()),\n",
    "        \"duplicate\": duplicate_flag\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "returning-industry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missing_target    0.258009\n",
       "duplicate         0.226840\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "computational-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    595\n",
       "1    560\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "identified-connection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-mountain",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "\n",
    "1. All 10 features describing the matching vector are loaded as floats.\n",
    "2. The 10 input features will be <b><u>treated as an output from a black box algorithm</u></b> for which we only can <b><u>assume that the relationship between the values of a feature is ordinal</u></b>\n",
    "3. <b><u>For 23% of the receipts we have entries with exactly the same input as the one for the correctly mapped transactions. For such cases the performance of the model will be based on the randomized way the transactions are provided. A non-discretized features should be requested</u></b>\n",
    "4. For 26% of the entries we don't have a matched transaction\n",
    "5. No missing values are observed\n",
    "6. No corrupted values are observed\n",
    "7. The columns are:\n",
    "- DateMappingMatch - A discretized matching similarity of the date returned by the upstream service\n",
    "- AmountMappingMatch - A discretized matching similarity of the amount returned by the upstream service\n",
    "- DescriptionMatch - A discretized matching similarity of the description returned by the upstream service\n",
    "- DifferentPredictedTime - A binary flag indicating if the image receipt time is same as transaction time\n",
    "- TimeMappingMatch - A discretized matching similarity of the time returned by the upstream service (0/1 observed)\n",
    "- PredictedNameMatch -  A discretized matching similarity of the predicted name returned by the upstream service\n",
    "- ShortNameMatch - A discretized matching similarity of the short name returned by the upstream servicee (0/1 observed)\n",
    "- DifferentPredictedDate - A binary flag indicating if the predicted date is the same\n",
    "- PredictedAmountMatch - A discretized matching similarity of the amount returned by the upstream servicee (0/1 observed)\n",
    "- PredictedTimeCloseMatch - A binary flag indicating if image receipt time is close to transaction time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-community",
   "metadata": {},
   "source": [
    "# 3. Get Target Flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-redhead",
   "metadata": {},
   "source": [
    "## 3.1. Derive Target Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "naval-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target\"] = (data[\"matched_transaction_id\"] == data[\"feature_transaction_id\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cheap-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11177\n",
       "1      857\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "checked-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Rate In data is 0.071\n"
     ]
    }
   ],
   "source": [
    "print(f\"Positive Rate In data is {round(data.target.mean(), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-involvement",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "1. Data Is Imbalanced with 7.1% of observations having positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-metabolism",
   "metadata": {},
   "source": [
    "# 4. Univiariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-barbados",
   "metadata": {},
   "source": [
    "## 4.1. Check Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chief-retailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateMappingMatch; Gini: 0.83\n",
      "AmountMappingMatch; Gini: 0.003\n",
      "DescriptionMatch; Gini: 0.229\n",
      "DifferentPredictedTime; Gini: -0.166\n",
      "TimeMappingMatch; Gini: 0.17\n",
      "PredictedNameMatch; Gini: 0.197\n",
      "ShortNameMatch; Gini: 0.241\n",
      "DifferentPredictedDate; Gini: -0.796\n",
      "PredictedAmountMatch; Gini: 0.012\n",
      "PredictedTimeCloseMatch; Gini: 0.224\n"
     ]
    }
   ],
   "source": [
    "float_columns = data.dtypes[data.dtypes == \"float\"].index.values\n",
    "\n",
    "for column in float_columns:\n",
    "    gini = round(roc_auc_score(data[\"target\"], data[column])*2 - 1, 3)\n",
    "    print(f\"{column}; Gini: {gini}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-investor",
   "metadata": {},
   "source": [
    "## 4.2. Check Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complex-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_correlation = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "saved-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = data[float_columns].corr().abs().stack().reset_index()\n",
    "corrs.columns = ['feature_1', 'feature_2', 'abs_corr']\n",
    "corrs = corrs[(corrs.feature_1 != corrs.feature_2) & (corrs[\"abs_corr\"] >= high_correlation)].sort_values(\"abs_corr\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "measured-tournament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>abs_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DateMappingMatch</td>\n",
       "      <td>DifferentPredictedDate</td>\n",
       "      <td>0.990860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>DifferentPredictedDate</td>\n",
       "      <td>DateMappingMatch</td>\n",
       "      <td>0.990860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>TimeMappingMatch</td>\n",
       "      <td>0.987785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TimeMappingMatch</td>\n",
       "      <td>DifferentPredictedTime</td>\n",
       "      <td>0.987785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature_1               feature_2  abs_corr\n",
       "7         DateMappingMatch  DifferentPredictedDate  0.990860\n",
       "70  DifferentPredictedDate        DateMappingMatch  0.990860\n",
       "34  DifferentPredictedTime        TimeMappingMatch  0.987785\n",
       "43        TimeMappingMatch  DifferentPredictedTime  0.987785"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-egypt",
   "metadata": {},
   "source": [
    "### 4.2.1. Analyze DifferentPredictedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eastern-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>DateMappingMatch</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.525</th>\n",
       "      <th>0.550</th>\n",
       "      <th>0.650</th>\n",
       "      <th>0.725</th>\n",
       "      <th>0.750</th>\n",
       "      <th>0.825</th>\n",
       "      <th>0.850</th>\n",
       "      <th>0.900</th>\n",
       "      <th>0.950</th>\n",
       "      <th>1.000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DifferentPredictedDate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>98</td>\n",
       "      <td>194</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>179</td>\n",
       "      <td>571</td>\n",
       "      <td>217</td>\n",
       "      <td>1636</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>9068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "DateMappingMatch        0.000  0.525  0.550  0.650  0.725  0.750  0.825  \\\n",
       "DifferentPredictedDate                                                    \n",
       "0.0                         0     11     98    194      3     21    179   \n",
       "1.0                      9068      0      0      0      0      0      0   \n",
       "\n",
       "DateMappingMatch        0.850  0.900  0.950  1.000  \n",
       "DifferentPredictedDate                              \n",
       "0.0                       571    217   1636     36  \n",
       "1.0                         0      0      0      0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data[\"DifferentPredictedDate\"], data[\"DateMappingMatch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-assignment",
   "metadata": {},
   "source": [
    "### 4.2.2. Analyze DifferentPredictedTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nuclear-hollywood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>TimeMappingMatch</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DifferentPredictedTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>11867</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "TimeMappingMatch          0.0  1.0\n",
       "DifferentPredictedTime            \n",
       "0.0                         0  163\n",
       "1.0                     11867    4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data[\"DifferentPredictedTime\"], data[\"TimeMappingMatch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-mississippi",
   "metadata": {},
   "source": [
    "## 4.3. Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "welsh-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame([dict(zip(float_columns, f_classif(data[float_columns], data[\"target\"])[0]))], index=[\"F-Value\"]).T.sort_values(\"F-Value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "retained-silly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DateMappingMatch</th>\n",
       "      <td>3911.125458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DifferentPredictedDate</th>\n",
       "      <td>3508.992917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeMappingMatch</th>\n",
       "      <td>1946.599561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DifferentPredictedTime</th>\n",
       "      <td>1909.256393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShortNameMatch</th>\n",
       "      <td>1412.235723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DescriptionMatch</th>\n",
       "      <td>1242.015363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PredictedNameMatch</th>\n",
       "      <td>1155.423875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PredictedTimeCloseMatch</th>\n",
       "      <td>593.635571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PredictedAmountMatch</th>\n",
       "      <td>72.983841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmountMappingMatch</th>\n",
       "      <td>1.004581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             F-Value\n",
       "DateMappingMatch         3911.125458\n",
       "DifferentPredictedDate   3508.992917\n",
       "TimeMappingMatch         1946.599561\n",
       "DifferentPredictedTime   1909.256393\n",
       "ShortNameMatch           1412.235723\n",
       "DescriptionMatch         1242.015363\n",
       "PredictedNameMatch       1155.423875\n",
       "PredictedTimeCloseMatch   593.635571\n",
       "PredictedAmountMatch       72.983841\n",
       "AmountMappingMatch          1.004581"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "attended-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_features = []\n",
    "for feature in table.index:\n",
    "    if feature not in corrs[\"feature_1\"].values or all([x not in explanatory_features for x in corrs[corrs[\"feature_1\"] == feature][\"feature_2\"].values]):\n",
    "        explanatory_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "revised-holocaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DateMappingMatch',\n",
       " 'TimeMappingMatch',\n",
       " 'ShortNameMatch',\n",
       " 'DescriptionMatch',\n",
       " 'PredictedNameMatch',\n",
       " 'PredictedTimeCloseMatch',\n",
       " 'PredictedAmountMatch',\n",
       " 'AmountMappingMatch']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanatory_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-drinking",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "1. The relationship between most faetures and the target flag is positive with 2 features having a negative relationship - DifferentPredictedTime, DifferentPredictedDate. All relationships follow the expected direction\n",
    "2. The supplied amount features are not expected to be very predictive in comparisson to the other features. Potentially we can remove them or depending on the classifier leave for them classifier to decide if they should be used\n",
    "3. There are 2 pairs of highly correlated features. An analysis of the features shows that the DateMappingMatch feature can be used to exactly reproduce the \"DifferentPredictedDate\" feature and \"TimeMappingMatch\" can be used to reproduce almost exactly the \"DifferentPredictedTime\" with just 4 observations that can't. As a result it was decided to remove the features with prefix \"Different\" from the training process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-transcription",
   "metadata": {},
   "source": [
    "# 5. Split In Train And Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "opponent-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = data.dtypes[data.dtypes == \"float\"].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "signal-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kfold = GroupShuffleSplit(test_size=0.25, random_state=21)\n",
    "groups = data[\"receipt_id\"]\n",
    "train_index, test_index = list(group_kfold.split(data.values, None, groups))[0]\n",
    "\n",
    "X_train = data.iloc[train_index, :][float_columns]\n",
    "y_train = data.iloc[train_index, :][\"target\"]\n",
    "group_col_train = data.iloc[train_index, :][\"receipt_id\"]\n",
    "X_test = data.iloc[test_index, :][float_columns]\n",
    "y_test = data.iloc[test_index, :][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "challenging-procurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8570\n",
       "1     639\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "israeli-blast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2607\n",
       "1     218\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-adventure",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "1. We split the data in 75% data for training and hyperparameter tuning and 25% hold-out sample for evaluation\n",
    "2. We use the receipt_id for grouping to ensure that the same receipt is not used in both train and test as all features are based on both the receipt_id and the feature_transaction_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-paris",
   "metadata": {},
   "source": [
    "# 6. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-internship",
   "metadata": {},
   "source": [
    "# 6.1. Initiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "working-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "scale_pos_weight = counter[0] / counter[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "chinese-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[(\"sel\", ColumnSelector(corr_threshold=0.90)),\n",
    "                        (\"cls\", XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=21))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-sending",
   "metadata": {},
   "source": [
    "# 6.2. Specify Hyperparamters and Initiate optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "statutory-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_fixed_params = {\n",
    "    \"cls__gamma\": [0, 0.5, 1, 3, 5],\n",
    "    \"cls__max_depth\": [3, 5, 10, 20],\n",
    "    \"cls__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"cls__reg_alpha\": [0, 0.5, 1, 3, 5],\n",
    "    \"cls__min_child_weight\": [10, 20, 30, 40],\n",
    "    \"cls__reg_lambda\": [0, 0.5, 1, 3, 5],\n",
    "    \"cls__n_estimators\": [10, 15, 20, 30, 40]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "described-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = list(GroupKFold(n_splits=4).split(X_train, y_train, group_col_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "professional-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RandomizedSearchCV(model,\n",
    "                               param_distributions=not_fixed_params,\n",
    "                               n_iter=300,\n",
    "                               scoring=\"roc_auc\",\n",
    "                               n_jobs=-1,\n",
    "                               cv=cv,\n",
    "                               random_state=21,\n",
    "                               refit=True,\n",
    "                               verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "external-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 300 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 589 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 661 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 698 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 737 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done 817 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done 858 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed:   34.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1034 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1081 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1177 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   38.8s finished\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "innocent-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.set_params(**optimizer.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "private-search",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sel', ColumnSelector()),\n",
       "                ('cls',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='', learning_rate=0.2,\n",
       "                               max_delta_step=0, max_depth=3,\n",
       "                               min_child_weight=10, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=40,\n",
       "                               n_jobs=0, num_parallel_tree=1, random_state=21,\n",
       "                               reg_alpha=0, reg_lambda=0.5,\n",
       "                               scale_pos_weight=13.411580594679187, subsample=1,\n",
       "                               tree_method='exact', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-wellington",
   "metadata": {},
   "source": [
    "## 6.3. Store Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "vulnerable-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/model.pkl', 'wb') as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-prior",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "1. As we don't know much about the imput features we are using a tree-based model.\n",
    "2. We are working with imbalanced dataset so a tree boosting algorithm is expected to perform better\n",
    "3. For tuning the parameters we use a simple randomized grid search. In the future we can improve this by utilizing the package hyperopt and its bayesian optimization approach.\n",
    "4. The final model is trained on the full training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-crest",
   "metadata": {},
   "source": [
    "# 7. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-south",
   "metadata": {},
   "source": [
    "## 7.1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "appropriate-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/model.pkl', 'rb') as fp:\n",
    "    model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ddebe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "legitimate-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EvaluateModel(\"target\", \"receipt_id\")\n",
    "metrics.fit(X_test, data.iloc[test_index, :][[\"receipt_id\", \"target\"]], pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-yahoo",
   "metadata": {},
   "source": [
    "## 7.2. Evaluate Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "considerable-pastor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362272950551749"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train, model.predict_proba(X_train)[:,1])*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "closing-farming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328132093200028"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, model.predict_proba(X_test)[:,1])*2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-rehabilitation",
   "metadata": {},
   "source": [
    "## 7.3. Check Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "super-elite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prob_bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[0.0, 0.1)</th>\n",
       "      <td>2</td>\n",
       "      <td>2116</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.1, 0.2)</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.2, 0.3)</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.3, 0.4)</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.4, 0.5)</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.5, 0.6)</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.6, 0.7)</th>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>0.150943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.7, 0.8)</th>\n",
       "      <td>54</td>\n",
       "      <td>377</td>\n",
       "      <td>0.143236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.8, 0.9)</th>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>0.204082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.9, 1.01)</th>\n",
       "      <td>142</td>\n",
       "      <td>170</td>\n",
       "      <td>0.835294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target                \n",
       "               sum count      mean\n",
       "prob_bin                          \n",
       "[0.0, 0.1)       2  2116  0.000945\n",
       "[0.1, 0.2)       2    14  0.142857\n",
       "[0.2, 0.3)       0    35  0.000000\n",
       "[0.3, 0.4)       0     2  0.000000\n",
       "[0.4, 0.5)       0     2  0.000000\n",
       "[0.5, 0.6)       0     7  0.000000\n",
       "[0.6, 0.7)       8    53  0.150943\n",
       "[0.7, 0.8)      54   377  0.143236\n",
       "[0.8, 0.9)      10    49  0.204082\n",
       "[0.9, 1.01)    142   170  0.835294"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics._metrics[\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-equivalent",
   "metadata": {},
   "source": [
    "## 7.4. Check Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "offshore-glenn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6089965397923875"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics._metrics[\"user_success_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "outstanding-knowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754325259515571"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics._metrics[\"max_success_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "recorded-resolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8073394495412844"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics._metrics[\"norm_success_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "skilled-bosnia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9058823529411765"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics._metrics[\"norm_no_dup_success_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-privacy",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "1. The discriminatory power of the model is very high with Gini on both Train and Test higher than 93%\n",
    "2. Distribution of the final score looks good, with observed values across the full range between 0 and 1\n",
    "3. The Final success rate is reported using 4 metrics:\n",
    "* \"User Success Rate\" - This is the success rate of visualziing the correct transaction in the final app as experienced by the user\n",
    "* \"Maximum Success Rate\" - This is the maximum success rate given that for a large proportion of the data we don't have the correct transaction supplied and assuming that when we have same explanatory features for 2 transactions, one of which is the correct one, we will select the correct one by chance\n",
    "* \"Normaized Success Rate\" - This is the success rate of the model after accounting for the receipts with missing matched transactions\n",
    "* \"Normaized For Missing and Duplicates Success Rate\" - This is the cussess rate after account for the receipts with missing matched transactions and after excluding all receipts for which we have same features for both the matched transaction and an incorrect transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-colleague",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-highway",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "receipt_36",
   "language": "python",
   "name": "receipt_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
